{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HPC_BettingNetworksEfficient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "176f55fc509b4ad587082cdbc043408f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ab623ad15134e0e967f17bd8f3ca3d0",
              "IPY_MODEL_ec122e74c2ad4eb4ab22dfcf47ddbd99",
              "IPY_MODEL_220a8bde78524d5a9360d09b645bf7d7"
            ],
            "layout": "IPY_MODEL_b32430c13c4d4aed8a629b06be7688de"
          }
        },
        "b32430c13c4d4aed8a629b06be7688de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ab623ad15134e0e967f17bd8f3ca3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_294fd49e960c4979b3055ece51f0761e",
            "placeholder": "​",
            "style": "IPY_MODEL_c21e6e88e26c48db89adf5b63ae03203",
            "value": ""
          }
        },
        "ec122e74c2ad4eb4ab22dfcf47ddbd99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e670d033cbb44de8c1fef5cc3950d2f",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7833ea021d0a456cba2c8eda36458b24",
            "value": 169001437
          }
        },
        "220a8bde78524d5a9360d09b645bf7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6595d203895446aa179b459616e675e",
            "placeholder": "​",
            "style": "IPY_MODEL_b2cfc1343fec42a495a09b15d72653ee",
            "value": " 169001984/? [00:12&lt;00:00, 16672452.15it/s]"
          }
        },
        "c21e6e88e26c48db89adf5b63ae03203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "294fd49e960c4979b3055ece51f0761e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7833ea021d0a456cba2c8eda36458b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e670d033cbb44de8c1fef5cc3950d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2cfc1343fec42a495a09b15d72653ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6595d203895446aa179b459616e675e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nverchev/BettingNetworks/blob/main/HPC_BettingNetworksEfficient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZp8zq17LyRf",
        "cellView": "form"
      },
      "source": [
        "#@title Do not share!\n",
        "from minio import Minio\n",
        "minioClient = Minio('nverchev.gaimfs.ugent.be',\n",
        "                  access_key='M12AX6PRAW2HUPUHPTL0',\n",
        "                  secret_key='yC2UlxBD+exlGz5S+zLaVclYUvRQ9D8msgOMVAWh',\n",
        "                  secure=True)\n",
        "dirpath = '/kyukon/data/gent/vo/001/gvo00118/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR4TmoIt0M-8"
      },
      "source": [
        "# Betting Networks\n",
        "This notebook is an implementation of Betting Networks, a classifier that uses adversarial learning. It comprises a baseline and evaluation metrics.\n",
        "\n",
        "To save and load the model please have a look at the trainer cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipYHmuEaMnWs",
        "cellView": "form"
      },
      "source": [
        "#@title Libraries\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from torch import optim\n",
        "import torch.cuda.amp as amp\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from abc import ABCMeta,abstractmethod\n",
        "import gc\n",
        "from sklearn import metrics\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision.datasets.cifar import CIFAR100 \n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL.Image import BICUBIC\n",
        "from collections import OrderedDict\n",
        "import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQX4J2ktNInv"
      },
      "source": [
        "#@title Hyperparameters: { display-mode: \"form\" }\n",
        "model_name = \"BaselineClassifier\"  #@param ['BaselineClassifier', 'BettingNetworks', 'BettingNetworksTwoHeaded', \"BaselineClassifier_no_momentum\", \"BettingCrossEntropy\"]\n",
        "version = 'MSE' #@param {type: \"string\"}\n",
        "batch_size =  64  #@param {type: \"number\"}\n",
        "noised_label_perc =  0# @param {type: \"integer\"}\n",
        "assert 0 <= noised_label_perc < 100\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "176f55fc509b4ad587082cdbc043408f",
            "b32430c13c4d4aed8a629b06be7688de",
            "3ab623ad15134e0e967f17bd8f3ca3d0",
            "ec122e74c2ad4eb4ab22dfcf47ddbd99",
            "220a8bde78524d5a9360d09b645bf7d7",
            "c21e6e88e26c48db89adf5b63ae03203",
            "294fd49e960c4979b3055ece51f0761e",
            "7833ea021d0a456cba2c8eda36458b24",
            "3e670d033cbb44de8c1fef5cc3950d2f",
            "b2cfc1343fec42a495a09b15d72653ee",
            "d6595d203895446aa179b459616e675e"
          ]
        },
        "id": "oIT7yLNoHm8-",
        "outputId": "f3d7e168-790e-49b3-e5c8-bf5daf8cf00f",
        "cellView": "form"
      },
      "source": [
        "#@title Create Datasets\n",
        "if 'dirpath' not in locals():\n",
        "  dirpath = \"./\"\n",
        "data_path = os.path.join(dirpath,'Cifar100')\n",
        "Loader = True #@param {type:\"boolean\"}\n",
        "def A_transform(transform):\n",
        "  return lambda img: transform(image=np.array(img))['image']\n",
        "\n",
        "def add_noise(noised_label_perc):\n",
        "  return lambda label: torch.tensor(label)\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.0, always_apply=True),\n",
        "    A.Sharpen((0.1,0.2)),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    A.SmallestMaxSize(256, interpolation=2),\n",
        "    A.Affine(shear=(-5,5)),\n",
        "    A.ShiftScaleRotate(rotate_limit=10, p=0.5),\n",
        "    A.RandomCrop(224,224),\n",
        "    A.CoarseDropout(max_holes=1, max_height=56, max_width=56, p=0.3),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "test_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),                          \n",
        "    A.SmallestMaxSize(224, interpolation=2),   \n",
        "    A.pytorch.ToTensorV2(),\n",
        "])\n",
        "split = 1/6\n",
        "pin_memory=torch.cuda.is_available()\n",
        "\n",
        "train_dataset = CIFAR100(root=data_path, train=True, transform=A_transform(train_transform), download=Loader)\n",
        "val_dataset =  CIFAR100(root=data_path, train=True, transform=A_transform(test_transform), download=False)\n",
        "test_dataset = CIFAR100(root=data_path, train=False, transform=A_transform(test_transform), download=Loader)\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "split = int(np.floor(split * num_train))\n",
        "indices = list(range(num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]    \n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# adding noise to targets during training\n",
        "if noised_label_perc > 0:\n",
        "  np.random.seed = 123 \n",
        "  jump = 100 // noised_label_perc\n",
        "  for i in range(0, len(train_dataset), jump):\n",
        "    train_dataset.targets[i] = np.random.randint(100)\n",
        "\n",
        "# class umbalanced\n",
        "if version == \"class_imbalanced\":\n",
        "  for i in range(len(train_dataset)):\n",
        "    train_dataset.targets[i] = min(49, train_dataset.targets[i])\n",
        "  for i in range(len(test_dataset)):\n",
        "    test_dataset.targets[i] = min(49, test_dataset.targets[i])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=batch_size, sampler=train_sampler, drop_last=True, pin_memory=pin_memory)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, sampler=valid_sampler, pin_memory=pin_memory\n",
        ")\n",
        "train_val_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=pin_memory\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                          shuffle=False, pin_memory=pin_memory)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTHLgPRmMjTU",
        "outputId": "6cc4f4d1-ed86-4e4a-af3a-88ebc7252b34"
      },
      "source": [
        "#@title Block Args\n",
        "initial_learning_rate = 0.01 if model_name == \"BaselineClassifier\" else 0.02\n",
        "weight_decay = 0.001\n",
        "\n",
        "class ExponentialSchedule:\n",
        "    def __init__(self, exp_decay = .975):\n",
        "        self.exp_decay = exp_decay\n",
        "\n",
        "    def __call__(self, base_lr, epoch):\n",
        "        return base_lr*self.exp_decay**epoch\n",
        "\n",
        "    def __repr__(self):\n",
        "        return  \"ExponentialSchedule\"\n",
        "\n",
        "class CosineSchedule:\n",
        "    def __init__(self, decay_steps = 60, min_decay = 0.1):\n",
        "        self.decay_steps = decay_steps\n",
        "        self.min_decay = min_decay\n",
        "\n",
        "    def __call__(self, base_lr, epoch):\n",
        "        min_lr = self.min_decay * base_lr\n",
        "        return min_lr + (base_lr - min_lr) * (1 + np.cos(np.pi * epoch/self.decay_steps)/ 2)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return  \"CosineSchedule\"\n",
        "\n",
        "momentum = 0.9 if model_name == \"BaselineClassifier\" else 0\n",
        "nesterov = model_name == \"BaselineClassifier\"\n",
        "opt = 'SGD'\n",
        "optimizer= { 'Adam': optim.Adam,\n",
        "             'SGD': optim.SGD,\n",
        "            'AdamW': optim.AdamW\n",
        "    }\n",
        "def helper_lr(base_learning_rate):\n",
        "  lr = {}\n",
        "  lr['stem'] = base_learning_rate * 0.1\n",
        "  lr['blocks'] = base_learning_rate * 0.1\n",
        "  \n",
        "  if model_name in [\"BaselineClassifier\",\"BaselineClassifier_no_momentum\"]:\n",
        "      lr['head'] = base_learning_rate * 0.2\n",
        "      lr['classifier'] = base_learning_rate\n",
        "  elif model_name in [\"BettingNetworks\", \"BettingCrossEntropy\"]:\n",
        "      lr['head'] = base_learning_rate * 0.2\n",
        "      lr['book'] = base_learning_rate\n",
        "      lr['bettor'] = base_learning_rate\n",
        "  elif model_name == \"BettingNetworksTwoHeaded\":\n",
        "      lr['head1'] = base_learning_rate * 0.2\n",
        "      lr['head2'] = base_learning_rate * 0.2\n",
        "      lr['book'] = base_learning_rate\n",
        "      lr['bettor'] = base_learning_rate\n",
        "  return lr\n",
        "\n",
        "\n",
        "lr = helper_lr(initial_learning_rate) \n",
        "\n",
        "# those args will be later overwritten in favour of finetuning\n",
        "optimi_args = {'Adam': {'weight_decay':weight_decay, 'lr': lr},\n",
        "               'SGD' : {'weight_decay':weight_decay, 'lr': lr,\n",
        "                        'momentum': momentum, 'nesterov':nesterov},\n",
        "               'AdamW': {'weight_decay':weight_decay, 'lr': lr},\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "train_loader = train_loader if version[:5]!= 'final' else train_val_loader\n",
        "val_loader = val_loader if version[:5]!= 'final' else None\n",
        "block_args={\n",
        "    'optim_name': opt,\n",
        "    'optim': optimizer[opt],\n",
        "    'optim_args': optimi_args[opt],    \n",
        "    'train_loader': train_loader,\n",
        "    'device': device,\n",
        "    'val_loader':  val_loader,\n",
        "    'test_loader': test_loader,\n",
        "    'batch_size': batch_size,\n",
        "    'schedule': CosineSchedule()\n",
        "    }\n",
        "for k,v in block_args.items():\n",
        "  if not isinstance(v,(type,torch.utils.data.dataloader.DataLoader)):\n",
        "    print(k, ': ', v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optim_name :  SGD\n",
            "optim_args :  {'weight_decay': 0.001, 'lr': {'stem': 0.001, 'blocks': 0.001, 'head': 0.002, 'classifier': 0.01}, 'momentum': 0.9, 'nesterov': True}\n",
            "device :  cuda:0\n",
            "val_loader :  None\n",
            "batch_size :  64\n",
            "schedule :  CosineSchedule\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyugo7AZOxTw",
        "cellView": "form"
      },
      "source": [
        "#@title Trainer\n",
        "'''\n",
        "This abstract class manages training and general utilites.\n",
        "It works together with a class defining the loss.\n",
        "This loss returns a dictionary dict with \n",
        "dict[\"Criterion\"] = *list of losses to backprop*\n",
        "\n",
        "To save and load, it expects a Minio object from the minio library.\n",
        "This object downloads and uploads the model to a separate storage.\n",
        "In order to get full access to this class utilities,\n",
        "set up MinIO on your storage device (https://min.io)\n",
        "install the minio api and then add the following:\n",
        "\n",
        "from minio import Minio\n",
        "minioClient = Minio(*Your storage name*,\n",
        "                  access_key= *Your access key*,\n",
        "                  secret_key= *Your secret key*,\n",
        "                  secure=True)\n",
        "\n",
        "'''\n",
        "\n",
        "class Trainer(metaclass=ABCMeta):\n",
        "    losses = [] # defiend later with the loss function\n",
        "    hypertuning_mode = False # less output\n",
        "    max_output = np.inf #maximum amount of stored evaluated test samples\n",
        "    minio = minioClient # None if you don't use minio\n",
        "    bin = 'bettingnetworksefficient' #minio bin\n",
        "    dirpath = dirpath \n",
        "    miniopath = staticmethod(lambda path: path[len(dirpath):]) #removes dirpath \n",
        "\n",
        "    def __init__(self,model,version,device,optim,train_loader,val_loader=None,\n",
        "                 test_loader=None, mp=False,**block_args):\n",
        "        \n",
        "        torch.manual_seed = 112358\n",
        "        self.epoch = 0\n",
        "        self.device = device # to cuda or not to cuda?\n",
        "        self.model = model.to(device) # model is not copied\n",
        "        self.version = version # name used for saving and loading  \n",
        "        self.schedule = block_args['schedule']\n",
        "        self.settings = {**model.settings,**block_args,'Optimizer':str(optim)}\n",
        "        self.optimizer_settings = block_args['optim_args'].copy()\n",
        "        self.optimizer = optim(**self.optimizer_settings)\n",
        "        self.mp = mp and device.type == 'cuda' # mixed precision casting\n",
        "        self.scaler = amp.GradScaler(enabled=mp) # mixed precision backpropa\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.train_losses = {loss:[] for loss in self.losses}\n",
        "        self.val_losses = {loss:[] for loss in self.losses}\n",
        "        self.test_losses = {loss:[] for loss in self.losses}\n",
        "        self.test_targets,self.test_outputs = [], {}\n",
        "        self.converge = 1 # if 0 kills session\n",
        "\n",
        "    @property\n",
        "    def optimizer_settings(self): #settings shown depend on epoch \n",
        "        if self.schedule is None:   \n",
        "          return {'params':self._optimizer_settings[0],\n",
        "                  **self._optimizer_settings[1]}\n",
        "        else: # the scheduler modifies the learning rate(s)\n",
        "          init_learning = self._optimizer_settings[0]\n",
        "          sheduled_learning = []\n",
        "          for group in init_learning:\n",
        "            sheduled_learning.append({\n",
        "                                      'params': group['params'],\n",
        "                                      'lr': self.schedule(group['lr'],self.epoch)\n",
        "                                      })\n",
        "          return  {'params':sheduled_learning, \n",
        "                   **self._optimizer_settings[1]}\n",
        " \n",
        "\n",
        "    @optimizer_settings.setter\n",
        "    def optimizer_settings(self, optim_args):\n",
        "        lr = optim_args.pop('lr')\n",
        "        if isinstance(lr,dict):\n",
        "            self._optimizer_settings = [\n",
        "                            {'params': getattr(self.model,k).parameters(),\n",
        "                              'lr':v} for k,v in lr.items()], optim_args\n",
        "        else:\n",
        "            self._optimizer_settings = \\\n",
        "                            [{'params':model.parameters(),'lr':lr}],optim_args\n",
        "        return        \n",
        "\n",
        "\n",
        "    def update_learning_rate(self, new_lr):\n",
        "        if not isinstance(new_lr,list): # transform to list\n",
        "          new_lr = [{'lr':new_lr} for _ in self.optimizer.param_groups]\n",
        "        for g, up_g in zip(self.optimizer.param_groups, new_lr):\n",
        "                g['lr'] = up_g['lr']\n",
        "        return\n",
        "\n",
        "\n",
        "    def train(self, num_epoch, val_after_train = False):\n",
        "        print('Version ',self.version)\n",
        "        for _ in range(num_epoch):\n",
        "            self.update_learning_rate(self.optimizer_settings['params'])\n",
        "            self.epoch += 1\n",
        "            print('====> Epoch:{}'.format(self.epoch))        \n",
        "            self._run_session(mode='train')\n",
        "            if self.val_loader and val_after_train: #check losses on val\n",
        "              self._run_session(mode='val')         #best to test instead\n",
        "        return\n",
        "  \n",
        "\n",
        "    def test(self, on = 'val'): #runs and stores evaluated test samples\n",
        "        print('Version ',self.version)\n",
        "        self.test_targets,self.test_outputs = \\\n",
        "            self._run_session(mode=on, save_outputs=True) #stored in RAM\n",
        "        return\n",
        "\n",
        "    def _run_session(self, mode='train', save_outputs=False, max_output=None):\n",
        "        if mode == 'train':\n",
        "            self.model.train()\n",
        "            torch.set_grad_enabled(True)\n",
        "            loader=self.train_loader\n",
        "            dict_losses= self.train_losses           \n",
        "        elif mode == 'val':\n",
        "            self.model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "            loader = self.val_loader\n",
        "            dict_losses= self.val_losses\n",
        "        elif mode == 'test':\n",
        "            self.model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "            loader = self.test_loader\n",
        "            dict_losses = self.test_losses\n",
        "        else:\n",
        "            raise ValueError('mode options are: \"train\", \"val\", \"test\" ')\n",
        "        if save_outputs:\n",
        "            test_targets,test_outputs = [], {}\n",
        "\n",
        "        len_sess = len(loader.dataset)\n",
        "        epoch_loss = {loss:0 for loss in  self.losses}\n",
        "        num_batch = len(loader)\n",
        "        iterable = enumerate(loader) if self. hypertuning_mode else \\\n",
        "                             tqdm(enumerate(loader),total=num_batch) \n",
        "        for batch_idx, (inputs, targets) in iterable:\n",
        "            if self.converge == 0:\n",
        "              return \n",
        "            inputs, targets = self.to_recursive([inputs,targets],device)\n",
        "            inputs_aux = self.helper_inputs(inputs,targets)\n",
        "            with amp.autocast(self.mp):\n",
        "                outputs =  self.model(**inputs_aux)\n",
        "                batch_loss = self.loss(outputs, inputs, targets)\n",
        "            criterion = batch_loss.pop('Criterion')\n",
        "            for loss in self.losses:\n",
        "               epoch_loss[loss] += batch_loss[loss].item()\n",
        "            if  mode == 'train':\n",
        "              for lss in criterion:\n",
        "                if torch.isinf(lss) or torch.isnan(lss):\n",
        "                  self.converge=0\n",
        "                self.scaler.scale(lss).backward()\n",
        "              self.scaler.step(self.optimizer)\n",
        "              self.scaler.update()\n",
        "              self.optimizer.zero_grad()\n",
        "            if save_outputs and \\\n",
        "                   self.max_output > (batch_idx + 1) * loader.batch_size:\n",
        "              self.extend_dict_list(\n",
        "                  test_outputs, self.to_recursive(outputs,'detach_cpu'))\n",
        "              test_targets.extend(self.to_recursive(targets,'detach_cpu'))\n",
        "            if not self.hypertuning_mode and mode=='train':\n",
        "                if batch_idx % (len(loader)//10 or 1) == 0:\n",
        "                    iterable.set_description(\n",
        "                                    'Train [{:4d}/{:4d} ]\\tLoss {:4f}'.format(\n",
        "                                                batch_idx * loader.batch_size,\n",
        "                                                len_sess,\n",
        "                                                criterion[0].item())\n",
        "                                            )\n",
        "                if batch_idx == len(loader)-1: # clear after last\n",
        "                    iterable.set_description('') \n",
        "\n",
        "        for loss in self.losses:\n",
        "          epoch_loss[loss] /= num_batch\n",
        "          if not save_outputs: #do not save history when testing\n",
        "              dict_losses[loss].append(epoch_loss[loss])\n",
        "        if not self.hypertuning_mode:\n",
        "          print('Average {} losses :'.format(mode))\n",
        "          for loss in self.losses:\n",
        "              print('{}: {:.4f}'.format(loss,epoch_loss[loss]), end='\\t')\n",
        "          print()\n",
        "        if save_outputs:\n",
        "          return  test_targets,test_outputs\n",
        "        else:\n",
        "          return\n",
        "        \n",
        "    def helper_inputs(self,inputs,labels):\n",
        "        return {'x' : inputs}\n",
        "\n",
        "    def plot_losses(self, loss):\n",
        "        tidy_loss = \" \".join([s.capitalize() for s in loss.split('_')])\n",
        "        epochs=np.arange(self.epoch)\n",
        "        plt.plot(epochs, self.train_losses[loss], label='train')\n",
        "        if self.val_loader:\n",
        "          plt.plot(epochs, self.val_losses[loss], label='val')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(tidy_loss)\n",
        "        plt.title(f\"{self.version}\")\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def to_recursive(obj, device): #changes device in dictionary and lists\n",
        "      if isinstance(obj, list):\n",
        "        obj = [Trainer.to_recursive(item, device) for item in obj]\n",
        "      elif isinstance(obj, dict):\n",
        "        obj = {k: Trainer.to_recursive(v, device) for k,v in obj.items()}\n",
        "      else:\n",
        "        try: \n",
        "          obj=obj.detach().cpu() if device == 'detach_cpu' else  obj.to(device)\n",
        "        except AttributeError:\n",
        "          raise ValueError(f'Datatype {type(obj)} does not contain tensors')\n",
        "      return obj\n",
        "\n",
        "    @staticmethod #extends lists in dictionaries\n",
        "    def extend_dict_list(old_dict, new_dict):\n",
        "      for key,value in new_dict.items():         \n",
        "          if key not in old_dict.keys():\n",
        "            old_dict[key]=[]\n",
        "            if isinstance(value,list):\n",
        "              for elem in value:\n",
        "                old_dict[key].append([])\n",
        "          if isinstance(value,list):\n",
        "            for elem, new_elem in zip( old_dict[key],value):\n",
        "              elem.extend(new_elem)\n",
        "          else: \n",
        "            old_dict[key].extend(value)\n",
        "    \n",
        "    @staticmethod #indexes a list (inside of a list) inside of a dictionary\n",
        "    def index_dict_list(dict_list,ind):\n",
        "      list_dict = {}\n",
        "      for k, v in dict_list.items():\n",
        "        if len(v) == 0 or isinstance(v[0],list):\n",
        "          new_v = []\n",
        "          for elem in v:\n",
        "            new_v.append(elem[ind].unsqueeze(0))\n",
        "        else:\n",
        "          new_v = v[ind].unsqueeze(0)\n",
        "        list_dict[k] = new_v\n",
        "      return list_dict\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss(self,output, inputs, targets):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def save(self,new_version=None):\n",
        "        self.model.eval()\n",
        "        paths=self.paths(new_version)\n",
        "        torch.save(self.model.state_dict(),paths['model'])\n",
        "        torch.save(self.optimizer.state_dict(),paths['optim'])\n",
        "        json.dump(self.train_losses, open(paths['train_hist'], 'w'))\n",
        "        json.dump(self.val_losses, open(paths['val_hist'], 'w'))\n",
        "        if self.minio is not None:\n",
        "          for file in paths.values():\n",
        "            self.minio.fput_object(self.bin,self.miniopath(file),file)\n",
        "        return\n",
        "    \n",
        "    def load(self, epoch=None):\n",
        "        directory=self.version\n",
        "        \n",
        "        if epoch is not None:\n",
        "          self.epoch = epoch\n",
        "        else:\n",
        "          past_epochs = [] #here it looks for the most recent model\n",
        "          if self.minio is not None:\n",
        "            for file in self.minio.list_objects(self.bin,recursive=True):\n",
        "                file_dir, *file_name = file.object_name.split(\"/\")\n",
        "                if file_dir == directory and file_name[0][:5]=='model':\n",
        "                  past_epochs.append(int(re.sub(\"\\D\", \"\",file_name[0])))\n",
        "          local_path = os.path.join(self.dirpath,self.version)\n",
        "          if os.path.exists(local_path):\n",
        "              for file in os.listdir(local_path):\n",
        "                if file[:5]=='model':\n",
        "                      past_epochs.append(int(re.sub(\"\\D\", \"\",file)))\n",
        "          if len(past_epochs) == 0:\n",
        "            print(\"No saved models found\")\n",
        "            return\n",
        "          else:\n",
        "              self.epoch = max(past_epochs)\n",
        "        paths = self.paths()\n",
        "        if self.minio is not None:\n",
        "          for file in paths.values():\n",
        "            self.minio.fget_object(self.bin,self.miniopath(file),file)\n",
        "        self.model.load_state_dict(torch.load(paths['model'],\n",
        "                                  map_location=torch.device(self.device)))        \n",
        "        self.optimizer.load_state_dict(torch.load(paths['optim'],\n",
        "                                      map_location=torch.device(self.device)))\n",
        "        self.train_losses = json.load(open(paths['train_hist']))\n",
        "        self.val_losses = json.load(open(paths['val_hist']))\n",
        "        print(\"Loaded: \",paths['model'])\n",
        "        return\n",
        "    def paths(self,new_version=None):\n",
        "        if new_version: # save a parallel version to work with\n",
        "          directory = os.path.join(self.dirpath,new_version)\n",
        "          id = self.epoch\n",
        "        else:\n",
        "          directory = os.path.join(self.dirpath,self.version)\n",
        "          id = self.epoch\n",
        "        if not os.path.exists(directory):\n",
        "            os.mkdir(directory)\n",
        "        paths = {}\n",
        "        paths['model'] = os.path.join(directory,f'model_epoch{id}.pt')\n",
        "        paths['optim'] = os.path.join(directory,f'optimizer_epoch{id}.pt')\n",
        "        paths['train_hist'] = os.path.join(directory,'train_losses.json')\n",
        "        paths['val_hist'] = os.path.join(directory,'val_losses.json')\n",
        "        return paths\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "93rASDxsjqFC"
      },
      "source": [
        "#@title Metrics\n",
        "class ClassificationMetrics():\n",
        "    _metrics = {}\n",
        "    wrong_indices = []\n",
        "    average = \"macro\"\n",
        "    \n",
        "    #overwrites Trainer method\n",
        "    def test(self, run=True, on='test', prob = 'book'):\n",
        "      print('Version ',self.version)\n",
        "      if run:\n",
        "          self.test_targets,self.test_outputs = \\\n",
        "                self._run_session(mode=on, save_outputs=True) #stored in RAM \n",
        "      if prob == 'book': #standard or book probabilities\n",
        "        self.test_probs = torch.stack(self.test_outputs['probs'])\n",
        "      elif 'q' not in self.test_outputs.keys():\n",
        "        print('Bettor probabilities not available')\n",
        "        return\n",
        "      elif prob == 'bettor': #bettor probabilities\n",
        "        self.test_probs = torch.stack(self.test_outputs['q'])\n",
        "      elif prob == 'mean':\n",
        "        q = torch.stack(self.test_outputs['q'])\n",
        "        probs = torch.stack(self.test_outputs['q'])\n",
        "        self.test_probs = (q + probs)/2\n",
        "      else:\n",
        "        print('prob = '+ prob + ' not defined') \n",
        "        return       \n",
        "      self.test_pred = torch.argmax(self.test_probs, dim = 1)\n",
        "      self.targets = torch.stack(self.test_targets)\n",
        "      right_pred = (self.test_pred == self.targets)\n",
        "      self.wrong_indices = torch.nonzero(~right_pred)\n",
        "      self.calculate_metrics()\n",
        "      return\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        self.test(on = 'val')\n",
        "        return self._metrics\n",
        "\n",
        "    def calculate_metrics(self, print_flag=True):\n",
        "\n",
        "        avg_type = self.average.capitalize() + ' ' if self.test_probs.size(1) > 1 else \"\"\n",
        "        #calculates common and also gives back the indices of the wrong guesses   \n",
        "\n",
        "        self._metrics['Accuracy'] = \\\n",
        "              metrics.accuracy_score(self.targets,self.test_pred)\n",
        "        self._metrics[avg_type + 'F1 Score'] = \\\n",
        "              metrics.f1_score(self.targets,self.test_pred,average=self.average)\n",
        "        self._metrics[avg_type + 'Jaccard Score'] = \\\n",
        "              metrics.jaccard_score(self.targets,\n",
        "                                    self.test_pred,average=self.average)\n",
        "        self._metrics[avg_type + 'AUC ROC'] = \\\n",
        "              metrics.roc_auc_score(self.targets,self.test_probs,\n",
        "                                 average=self.average,multi_class='ovr')\n",
        "        if print_flag:\n",
        "          for metric, value in self._metrics.items():\n",
        "            print( metric + f' : {value:.4f}', end = '\\t' )\n",
        "          print('')\n",
        "        return\n",
        "    \n",
        "\n",
        "class Analysis():\n",
        "    '''\n",
        "    We consider here only the probability for the most likely class \n",
        "    (the max probability) as it is the more interesting for prediction.\n",
        "    '''\n",
        "\n",
        "    def prob_analysis(self,on = 'val', bins=100): #call after test\n",
        "        global wrong_hist, right_hist, hist\n",
        "        print(self.version)\n",
        "        if self.wrong_indices == []:\n",
        "          self.test(on=on)\n",
        "        confidence = np.linspace(1/(2*bins), 1-1/(2*bins), bins)\n",
        "        highest_conf = torch.max(self.test_probs, dim = 1)[0]\n",
        "        wrong_conf = highest_conf[self.wrong_indices]\n",
        "        hist = torch.histc(highest_conf, bins=bins, min=0, max=1)\n",
        "        wrong_hist = torch.histc(wrong_conf, bins=bins, min=0, max=1)\n",
        "        right_hist = hist - wrong_hist\n",
        "\n",
        "        # Empty bins are given expected probabability\n",
        "        obs_prob = np.divide(right_hist, hist,\\\n",
        "                out=np.zeros(bins), where=hist!=0)\n",
        "        print('Max probs')\n",
        "        plt.figure(figsize=(50,10))\n",
        "        plt.subplot(1, 4, 1)\n",
        "        plt.bar(confidence, hist/hist.sum(), width=.9/bins)\n",
        "        plt.xticks(np.linspace(0,1,5),np.linspace(0,1,5))\n",
        "        plt.title(self.version+'max probability distribution')\n",
        "\n",
        "        plt.subplot(1, 4, 2)\n",
        "        plt.bar(confidence, wrong_hist/wrong_hist.sum(), width=.9/bins)\n",
        "        plt.title('missclassified by prob (normalized)')\n",
        "        plt.xticks(np.linspace(0,1,5),np.linspace(0,1,5))\n",
        "\n",
        "        plt.subplot(1, 4, 3)\n",
        "        plt.bar(confidence, right_hist/right_hist.sum(), width=.9/bins)\n",
        "        plt.title('correct by prob (normalized)')\n",
        "        plt.xticks(np.linspace(0,1,5),np.linspace(0,1,5))\n",
        "        \n",
        "        plt.subplot(1, 4, 4)\n",
        "        plt.title('frequentist probability by conf')\n",
        "        plt.bar(confidence, obs_prob, width=.9/bins)\n",
        "        plt.xticks(np.linspace(0,1,5),np.linspace(0,1,5)) \n",
        "        plt.plot(np.linspace(0,1,100),np.linspace(0,1,100),color='b')\n",
        "        plt.show()\n",
        "        \n",
        "        targets = F.one_hot(self.targets, num_classes=self.model.num_classes).float()\n",
        "        brier_multi = torch.mean(torch.sum((self.test_probs - targets)**2, axis=1))\n",
        "        nonzero = np.nonzero(hist)\n",
        "        ece = np.abs(obs_prob[nonzero]-confidence[nonzero]).mean()\n",
        "        print('ECE_pred: ', ece, ' Brier: ', brier_multi.item())\n",
        "        ece = 0\n",
        "        for cls in range(self.model.num_classes):\n",
        "          conf = self.test_probs[:,cls]\n",
        "          right_indices = torch.nonzero(self.targets==cls)\n",
        "          right_conf = conf[right_indices]\n",
        "          hist = torch.histc(conf, bins=bins, min=0, max=1)\n",
        "          right_hist = torch.histc(right_conf, bins=bins, min=0, max=1)\n",
        "          wrong_hist = hist - right_hist\n",
        "          nonzero = np.nonzero(hist)\n",
        "          obs_prob = np.divide(right_hist, hist, where=hist!=0)\n",
        "          ece += np.abs(obs_prob[nonzero]-confidence[nonzero]).mean()\n",
        "        print('ECE: ', ece/self.model.num_classes)\n",
        "        return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z86bgExUWBpY",
        "cellView": "form"
      },
      "source": [
        "#@title Utils, Layers and Blocks\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.flatten(1)\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    \n",
        "    def __init__(self, inplanes, se_planes):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        self.reduce_expand = nn.Sequential(\n",
        "            nn.Conv2d(inplanes, se_planes, \n",
        "                      kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            Swish(),\n",
        "            nn.Conv2d(se_planes, inplanes, \n",
        "                      kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_se = torch.mean(x, dim=(-2, -1), keepdim=True)\n",
        "        x_se = self.reduce_expand(x_se)\n",
        "        return x_se * x\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, planes, kernel_size, stride, \n",
        "                 expand_rate=1.0, se_rate=0.25, \n",
        "                 drop_connect_rate=0.2):\n",
        "        super(MBConv, self).__init__()\n",
        "\n",
        "        expand_planes = int(inplanes * expand_rate)\n",
        "        se_planes = max(1, int(inplanes * se_rate))\n",
        "\n",
        "        self.expansion_conv = None        \n",
        "        if expand_rate > 1.0:\n",
        "            self.expansion_conv = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, expand_planes, \n",
        "                          kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),\n",
        "                Swish()\n",
        "            )\n",
        "            inplanes = expand_planes\n",
        "\n",
        "        self.depthwise_conv = nn.Sequential(\n",
        "            nn.Conv2d(inplanes, expand_planes,\n",
        "                      kernel_size=kernel_size, stride=stride, \n",
        "                      padding=kernel_size // 2, groups=expand_planes,\n",
        "                      bias=False),\n",
        "            nn.BatchNorm2d(expand_planes, momentum=0.01, eps=1e-3),\n",
        "            Swish()\n",
        "        )\n",
        "\n",
        "        self.squeeze_excitation = SqueezeExcitation(expand_planes, se_planes)\n",
        "        \n",
        "        self.project_conv = nn.Sequential(\n",
        "            nn.Conv2d(expand_planes, planes, \n",
        "                      kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(planes, momentum=0.01, eps=1e-3),\n",
        "        )\n",
        "\n",
        "        self.with_skip = stride == 1\n",
        "        self.drop_connect_rate = drop_connect_rate\n",
        "    \n",
        "    def _drop_connect(self, x):        \n",
        "        keep_prob = 1.0 - self.drop_connect_rate\n",
        "        drop_mask = torch.rand(x.shape[0], 1, 1, 1) + keep_prob\n",
        "        drop_mask = drop_mask.type_as(x)\n",
        "        drop_mask.floor_()\n",
        "        return drop_mask * x / keep_prob\n",
        "        \n",
        "    def forward(self, x):\n",
        "        z = x\n",
        "        if self.expansion_conv is not None:\n",
        "            x = self.expansion_conv(x)\n",
        "\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.squeeze_excitation(x)\n",
        "        x = self.project_conv(x)\n",
        "        \n",
        "        # Add identity skip\n",
        "        if x.shape == z.shape and self.with_skip:            \n",
        "            if self.training and self.drop_connect_rate is not None:\n",
        "                x = self._drop_connect(x)\n",
        "            x += z\n",
        "        return x\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvvNBe7zN8ou",
        "cellView": "form"
      },
      "source": [
        "#@title EfficientNet\n",
        "def init_weights(module):    \n",
        "    if isinstance(module, nn.Conv2d):    \n",
        "        nn.init.kaiming_normal_(module.weight, a=0, mode='fan_out')\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        init_range = 1.0 / math.sqrt(module.weight.shape[1])\n",
        "        nn.init.uniform_(module.weight, a=-init_range, b=init_range)\n",
        "        \n",
        "        \n",
        "class EfficientNet(nn.Module):\n",
        "        \n",
        "    def _setup_repeats(self, num_repeats):\n",
        "        return int(math.ceil(self.depth_coefficient * num_repeats))\n",
        "    \n",
        "    def _setup_channels(self, num_channels):\n",
        "        num_channels *= self.width_coefficient\n",
        "        new_num_channels = math.floor(num_channels / self.divisor + 0.5) * self.divisor\n",
        "        new_num_channels = max(self.divisor, new_num_channels)\n",
        "        if new_num_channels < 0.9 * num_channels:\n",
        "            new_num_channels += self.divisor\n",
        "        return new_num_channels\n",
        "\n",
        "    def __init__(self, num_classes=100, \n",
        "                 width_coefficient=1.0,\n",
        "                 depth_coefficient=1.0,\n",
        "                 se_rate=0.25,\n",
        "                 dropout_rate=0.2,\n",
        "                 drop_connect_rate=0.2,\n",
        "                 list_channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.settings = {}\n",
        "        self.width_coefficient = width_coefficient\n",
        "        self.depth_coefficient = depth_coefficient\n",
        "        self.divisor = 8\n",
        "        self.num_classes = num_classes\n",
        "                \n",
        "        \n",
        "        list_channels = [self._setup_channels(c) for c in list_channels]\n",
        "\n",
        "        list_num_repeats = [1, 2, 2, 3, 3, 4, 1]\n",
        "        list_num_repeats = [self._setup_repeats(r) for r in list_num_repeats]        \n",
        "        \n",
        "        expand_rates = [1, 6, 6, 6, 6, 6, 6]\n",
        "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
        "        kernel_sizes = [3, 3, 5, 3, 5, 5, 3]\n",
        "\n",
        "        # Define stem:\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, list_channels[0], kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(list_channels[0], momentum=0.01, eps=1e-3),\n",
        "            Swish()\n",
        "        )\n",
        "        \n",
        "        # Define MBConv blocks\n",
        "        blocks = []\n",
        "        counter = 0\n",
        "        num_blocks = sum(list_num_repeats)\n",
        "        for idx in range(7):\n",
        "            \n",
        "            num_channels = list_channels[idx]\n",
        "            next_num_channels = list_channels[idx + 1]\n",
        "            num_repeats = list_num_repeats[idx]\n",
        "            expand_rate = expand_rates[idx]\n",
        "            kernel_size = kernel_sizes[idx]\n",
        "            stride = strides[idx]\n",
        "            drop_rate = drop_connect_rate * counter / num_blocks\n",
        "            \n",
        "            name = \"MBConv{}_{}\".format(expand_rate, counter)\n",
        "            blocks.append((\n",
        "                name,\n",
        "                MBConv(num_channels, next_num_channels, \n",
        "                       kernel_size=kernel_size, stride=stride, expand_rate=expand_rate, \n",
        "                       se_rate=se_rate, drop_connect_rate=drop_rate)\n",
        "            ))\n",
        "            counter += 1\n",
        "            for i in range(1, num_repeats):                \n",
        "                name = \"MBConv{}_{}\".format(expand_rate, counter)\n",
        "                drop_rate = drop_connect_rate * counter / num_blocks                \n",
        "                blocks.append((\n",
        "                    name,\n",
        "                    MBConv(next_num_channels, next_num_channels, \n",
        "                           kernel_size=kernel_size, stride=1, expand_rate=expand_rate, \n",
        "                           se_rate=se_rate, drop_connect_rate=drop_rate)                                    \n",
        "                ))\n",
        "                counter += 1\n",
        "        \n",
        "        self.blocks = nn.Sequential(OrderedDict(blocks))\n",
        "        \n",
        "        self.head_gen = lambda: nn.Sequential(\n",
        "                                      nn.Conv2d(list_channels[-2], list_channels[-1], \n",
        "                                                kernel_size=1, bias=False),\n",
        "                                      nn.BatchNorm2d(list_channels[-1], momentum=0.01, eps=1e-3),\n",
        "                                      Swish(),\n",
        "                                      nn.AdaptiveAvgPool2d(1),\n",
        "                                      Flatten()\n",
        "                                  )\n",
        "        self.classifier_gen = lambda: nn.Sequential(nn.Dropout(p=dropout_rate),\n",
        "                                      nn.Linear(list_channels[-1], num_classes)\n",
        "                                  )\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.blocks(x)\n",
        "        return x\n",
        "\n",
        "class BaselineClassifier(EfficientNet):\n",
        "\n",
        "    def __init__(self,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.head = self.head_gen()\n",
        "        self.classifier = self.classifier_gen()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        x = self.head(x)        \n",
        "        y = self.classifier(x)\n",
        "        return {'y' : y,\n",
        "                'probs': F.softmax(y,dim=1)}\n",
        "\n",
        "\n",
        "\n",
        "class BettingNetworks(EfficientNet):\n",
        "\n",
        "    def __init__(self,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.head = self.head_gen()\n",
        "        self.book = self.classifier_gen()\n",
        "        self.bettor = self.classifier_gen()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        x = self.head(x)        \n",
        "        hidden = x.detach().clone()\n",
        "        y = self.book(x)\n",
        "        p = F.softmax(y,dim=1)  \n",
        "        yhat = self.bettor(hidden)\n",
        "        q = F.softmax(yhat,dim=1)\n",
        "        return {'y': y,\n",
        "                'yhat': yhat,\n",
        "                'probs': p,\n",
        "                'q': q\n",
        "                }\n",
        "\n",
        "if model_name == \"BaselineClassifier\":\n",
        "    model = BaselineClassifier()\n",
        "elif model_name =='BettingNetworks':\n",
        "    model = BettingNetworks()\n",
        "\n",
        "def init_weights(module):    \n",
        "    if isinstance(module, nn.Conv2d):    \n",
        "        nn.init.kaiming_normal_(module.weight, a=0, mode='fan_out')\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        init_range = 1.0 / math.sqrt(module.weight.shape[1])\n",
        "        nn.init.uniform_(module.weight, a=-init_range, b=init_range)\n",
        "    \n",
        "\n",
        "class BettingNetworksTwoHeaded(EfficientNet):\n",
        "\n",
        "    def __init__(self,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.head1 = self.head_gen()\n",
        "        self.book = self.classifier_gen()\n",
        "        self.head2 = self.head_gen()\n",
        "        self.bettor = self.classifier_gen()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        detached = x.detach().clone()\n",
        "        x = self.head1(x)\n",
        "        y = self.book(x)\n",
        "        p = F.softmax(y,dim=1)\n",
        "        x = self.head2(detached)\n",
        "        yhat = self.bettor(x)\n",
        "        q = F.softmax(yhat,dim=1)\n",
        "        return {'y': y,\n",
        "                'yhat': yhat,\n",
        "                'probs': p,\n",
        "                'q': q\n",
        "                }\n",
        "model_class = {\"BaselineClassifier\" : BaselineClassifier,\n",
        "              \"BettingNetworks\" : BettingNetworks,\n",
        "              \"BettingNetworksTwoHeaded\": BettingNetworksTwoHeaded\n",
        "              }\n",
        "num_classes = 50 if version == \"class_imbalanced\" else 100\n",
        "model = model_class[model_name](num_classes=num_classes)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bG-wilOC9yB"
      },
      "source": [
        "#@title Losses\n",
        "\n",
        "class CELoss():\n",
        "    losses=['CE']\n",
        "    def loss(self, output, inputs, targets):\n",
        "        y = output['y']\n",
        "        CE = F.cross_entropy(y,targets) \n",
        "        return {'Criterion':[CE],\n",
        "                'CE':CE}\n",
        "\n",
        "class MSELoss():\n",
        "\n",
        "    losses=['MSE']\n",
        "    def loss(self, output, inputs, targets):\n",
        "        probs = output['probs']\n",
        "        targets = F.one_hot(targets, num_classes=self.model.num_classes).float()\n",
        "        MSE = F.mse_loss(probs,targets) \n",
        "        return {'Criterion':[MSE],\n",
        "                'MSE':MSE,\n",
        "                }\n",
        "\n",
        "class NaiveBettingLoss():\n",
        "\n",
        "    losses=['Naive']\n",
        "    def loss(self, output, inputs, targets):\n",
        "        probs = output['probs']\n",
        "        #  counteracts bias in loss\n",
        "        #probs = (probs + 1/self.model.num_classes) / 2 \n",
        "        targets = F.one_hot(targets, num_classes=self.model.num_classes).float()\n",
        "        naive = ((1/self.model.num_classes-probs)*(targets-probs)).sum()\n",
        "        return {'Criterion':[naive],\n",
        "                'Naive':naive}\n",
        "\n",
        "class NewLoss1():\n",
        "\n",
        "    losses=['New Loss']\n",
        "    def loss(self, output, inputs, targets):\n",
        "        probs = output['probs']\n",
        "        targets = F.one_hot(targets, num_classes=self.model.num_classes).float()\n",
        "        New = ((probs)*(probs-2*targets)).sum()\n",
        "        return {'Criterion':[New],\n",
        "                'New Loss':New}\n",
        "\n",
        "                \n",
        "class BettingLoss():\n",
        "\n",
        "    '''\n",
        "    This loss comes with a neural network, which is defined globally.\n",
        "    The NN makes a bet while frezing the proposed probabilities.\n",
        "    The NN is updated before being used as a loss.\n",
        "    It is necessary to do it separately in two runs as the considered gradients are different.\n",
        "    '''\n",
        "\n",
        "    losses=['Book Loss','Bettor Loss',\"CEp\",\"CEq\"]\n",
        "\n",
        "    def loss(self, output, inputs, targets, eps = 0):\n",
        "        y = output['y']\n",
        "        yhat = output['yhat']\n",
        "        CEp = F.cross_entropy(y,targets)\n",
        "        CEq = F.cross_entropy(yhat,targets)\n",
        "        targets = F.one_hot(targets, num_classes=self.model.num_classes).float()\n",
        "        probs = output['probs']\n",
        "        p_detached = probs.detach()\n",
        "        q = output['q']\n",
        "        bettor_loss = ((q - p_detached)*(p_detached-targets-eps)).sum()\n",
        "        book_loss = ((q.detach() - probs)*(targets-probs-eps)).sum()\n",
        "        backprop = [book_loss,bettor_loss] \n",
        "        return {'Criterion':backprop,\n",
        "                'Book Loss':book_loss,\n",
        "                'Bettor Loss': bettor_loss,\n",
        "                'CEp': CEp,\n",
        "                'CEq': CEq\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw15NqiQJzkw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "aea3a8e1-79b2-44a5-a3c5-389a17fab873"
      },
      "source": [
        "#@title Classifier Training\n",
        "class ClassifierTrainer(MSELoss,Analysis,ClassificationMetrics,Trainer):\n",
        "    def __init__(self, model,version,block_args):\n",
        "        super().__init__(model,version,**block_args)\n",
        "        return \n",
        "\n",
        "class BettingNetworksTrainer(BettingLoss ,ClassifierTrainer):\n",
        "      pass\n",
        "\n",
        "\n",
        "if model_name in [\"BaselineClassifier\",\"BaselineClassifier_no_momentum\"]:\n",
        "    trainer = ClassifierTrainer(model,model_name + '_' + version,block_args)\n",
        "\n",
        "else:\n",
        "    trainer = BettingNetworksTrainer(model,model_name + '_' + version,block_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-66d86c61c86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Classifier Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mClassifierTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNaiveBettingLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAnalysis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mClassificationMetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mblock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NaiveBettingLoss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEY1s9FeHMiz",
        "cellView": "form",
        "outputId": "84876ad9-0a19-4f72-f5a7-6bc977861400"
      },
      "source": [
        "#@title Load weights\n",
        "\n",
        "efficient_path = os.path.join(dirpath,'EfficientNet')\n",
        "B0_state = torch.load(os.path.join(efficient_path,\"efficientnet-b0-08094119.pth\"))\n",
        "\n",
        "# A basic remapping is required\n",
        "mapping = {\n",
        "  k: v for k, v in zip(B0_state.keys(), model.state_dict().keys())\n",
        "          }\n",
        "\n",
        "mapped_B0_state = OrderedDict([\n",
        "          (mapping[k], v) for k, v in B0_state.items()\n",
        "                              ])\n",
        "# the last layer has different dimensions\n",
        "mapped_B0_state.popitem()\n",
        "mapped_B0_state.popitem()\n",
        "\n",
        "mapped_model_state = OrderedDict() \n",
        "for key, value in mapped_B0_state.items():\n",
        "    if key[:5] == 'head1': \n",
        "        # has effect only with TwoHeaded Betting Networks\n",
        "        layername = key[5:]\n",
        "        mapped_model_state['head2'+layername] = value\n",
        "        mapped_model_state[key] = value\n",
        "    else:\n",
        "      mapped_model_state[key] = value\n",
        "\n",
        "model.load_state_dict(mapped_model_state, strict=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['classifier.1.weight', 'classifier.1.bias'], unexpected_keys=[])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE3wcv4AJ1dA"
      },
      "source": [
        "#@title Training\n",
        "#trainer.load(39)\n",
        "#trainer.test(on='val')\n",
        "for _ in range(60):\n",
        "  trainer.train(1)\n",
        "  trainer.save()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zn5yPWxuyyf"
      },
      "source": [
        "# #@title Baseline classifier hyperparameters optimization\n",
        "# trainer.save() #save initial setup\n",
        "# def objective(trial):\n",
        "#     trainer.load(0) \n",
        "#     base_lr = trial.suggest_float(\"lr_init\", 1e-3, 1e-1, log=True)\n",
        "#     print(f\"New learning rate init = {base_lr}\")\n",
        "#     lr = helper_lr(base_lr) # scales the learning rate proportionally\n",
        "#     trainer.update_learning_rate(lr)\n",
        "#     for step in range(10):\n",
        "#         trainer.train(1)\n",
        "#         trainer.test(on='test')\n",
        "#         intermediate_value = trainer.metrics['Accuracy']\n",
        "#         trial.report(intermediate_value, step) #only one check\n",
        "#         if trial.should_prune():\n",
        "#             raise optuna.TrialPruned()\n",
        "#     return trainer.metrics['Accuracy']\n",
        "\n",
        "# trainer.hypertuning_mode = True\n",
        "# study_name = 'tune_lr'  # Unique identifier of the study.\n",
        "# study = optuna.create_study(\n",
        "#                 study_name=study_name,\n",
        "#                 direction='maximize',\n",
        "#                 pruner=optuna.pruners.MedianPruner(n_startup_trials=3,\n",
        "#                                                    n_warmup_steps=2))\n",
        "# study.optimize(objective, n_trials=10)\n",
        "\n",
        "# trial = study.best_trial\n",
        "\n",
        "# print('Accuracy: {}'.format(trial.value))\n",
        "# print(\"Best hyperparameters: {}\".format(trial.params))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}